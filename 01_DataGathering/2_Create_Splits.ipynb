{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Splitting\n",
    "\n",
    "In this Notebook the Split of the dataset is conducted. Different Annotation files are created but they all use the same split ratio with the same methods so the split assciation is equal for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import random \n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('../00_Dataset/annotations.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_ratio = 0.7\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "test_only_clips = [(\"Video_6\",\"clip_1\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: Amateur\n",
      "Split: train, total frames: 7889, ratio: 0.6835629494844467\n",
      "Split: validation, total frames: 1689, ratio: 0.1463478034832337\n",
      "Split: test, total frames: 1963, ratio: 0.17008924703231956\n",
      "\n",
      "Subset: TrackNet\n",
      "Split: train, total frames: 13947, ratio: 0.7031509957146458\n",
      "Split: validation, total frames: 2984, ratio: 0.1504411394000504\n",
      "Split: test, total frames: 2904, ratio: 0.14640786488530375\n",
      "\n",
      "Subset: Court\n",
      "Split: train, total frames: 6312, ratio: 0.7139463861554123\n",
      "Split: validation, total frames: 1164, ratio: 0.13165931455717678\n",
      "Split: test, total frames: 1365, ratio: 0.1543942992874109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a copy of the data\n",
    "entire_dataset = copy.deepcopy(data)\n",
    "\n",
    "def assign_splits(dataset):\n",
    "    for subset in dataset[\"subsets\"]:\n",
    "        for video in subset[\"videos\"]:\n",
    "                num_frames = 0\n",
    "                \n",
    "                for clip in video['clips']:\n",
    "                    num_frames += len(clip['frames_with_objects'].keys())+1\n",
    "\n",
    "                num_train = int(num_frames * train_ratio)\n",
    "                num_validation = int(num_frames * validation_ratio)\n",
    "                num_test = num_frames - num_train - num_validation\n",
    "                \n",
    "                for clip in video['clips']:\n",
    "                    for frame_number, frame in clip['frames_with_objects'].items():\n",
    "                        if num_train > 0:\n",
    "                            frame['split'] = 'train'\n",
    "                            num_train -= 1\n",
    "                        elif num_validation > 0:\n",
    "                            frame['split'] = 'validation'\n",
    "                            num_validation -= 1\n",
    "                        elif num_test > 0:\n",
    "                            frame['split'] = 'test'\n",
    "                            num_test -= 1\n",
    "                        else:\n",
    "                            frame['split'] = 'none'\n",
    "\n",
    "                        # manage special test clips\n",
    "                        if (video['name'], clip['name']) in test_only_clips:\n",
    "                            frame['split'] = 'test'\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def count_splits(dataset):\n",
    "    # count frames for each split and each subset\n",
    "    dict={}\n",
    "    for subset in dataset[\"subsets\"]:\n",
    "        dict[subset[\"name\"]] = {\"train\": 0, \"validation\": 0, \"test\": 0}\n",
    "        for video in subset[\"videos\"]:\n",
    "            for clip in video['clips']:\n",
    "                for frame_number, frame in clip['frames_with_objects'].items():\n",
    "                    dict[subset[\"name\"]][frame['split']] += 1\n",
    "\n",
    "    for subset in dataset[\"subsets\"]:\n",
    "        total = sum(dict[subset[\"name\"]].values())\n",
    "        print(f\"Subset: {subset['name']}\")\n",
    "        for split in dict[subset[\"name\"]]:\n",
    "            # print total amount and ratio\n",
    "            print(f\"Split: {split}, total frames: {dict[subset['name']][split]}, ratio: {dict[subset['name']][split]/total}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# assign splits\n",
    "entire_dataset = assign_splits(entire_dataset)\n",
    "\n",
    "# count splits\n",
    "count_splits(entire_dataset)\n",
    "\n",
    "# safe annotations\n",
    "with open('../00_Dataset/annotations_complete.json', 'w') as f:\n",
    "    json.dump(entire_dataset, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ball Tracking Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: Amateur\n",
      "Split: train, total frames: 7889, ratio: 0.6835629494844467\n",
      "Split: validation, total frames: 1689, ratio: 0.1463478034832337\n",
      "Split: test, total frames: 1963, ratio: 0.17008924703231956\n",
      "\n",
      "Subset: TrackNet\n",
      "Split: train, total frames: 4342, ratio: 0.7020210185933711\n",
      "Split: validation, total frames: 929, ratio: 0.1502021018593371\n",
      "Split: test, total frames: 914, ratio: 0.14777687954729182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a copy of the data\n",
    "ball_dataset = copy.deepcopy(data)\n",
    "\n",
    "# assign splits\n",
    "ball_dataset = assign_splits(ball_dataset)\n",
    "\n",
    "# remove all non clay court games \n",
    "games_to_keep = [\"game4\", \"game6\", \"game8\"]\n",
    "\n",
    "for subset in ball_dataset[\"subsets\"]:\n",
    "    if subset[\"name\"] == \"TrackNet\":\n",
    "        subset[\"videos\"] = [video for video in subset[\"videos\"] if video[\"name\"] in games_to_keep]\n",
    "\n",
    "#remove third dataset by index\n",
    "ball_dataset[\"subsets\"].pop(2)\n",
    "\n",
    "# count splits\n",
    "count_splits(ball_dataset)\n",
    "\n",
    "# safe annotations\n",
    "with open('../00_Dataset/annotations_ball.json', 'w') as f:\n",
    "    json.dump(ball_dataset, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for Court detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Clustering to extract court color to filter for only clay courts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dominant_color(image_path):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert the image to RGB (in case it's not already in RGB mode)\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    # Get image dimensions\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Define the center region (e.g., a 200x200 square in the center)\n",
    "    center_x, center_y = width // 2, height // 2\n",
    "    region_size = 100  # Size of the center region for sampling\n",
    "    \n",
    "    # Define the bounds of the center region\n",
    "    left = max(center_x - region_size // 2, 0)\n",
    "    upper = max(center_y - region_size // 2, 0)\n",
    "    right = min(center_x + region_size // 2, width)\n",
    "    lower = min(center_y + region_size // 2, height)\n",
    "    \n",
    "    # Crop the center region and get pixel data as a NumPy array\n",
    "    center_region = image.crop((left, upper, right, lower))\n",
    "    center_pixels = np.array(center_region)\n",
    "    \n",
    "    # Flatten the pixel array and randomly select 100 pixels\n",
    "    center_pixels_flat = center_pixels.reshape(-1, 3)\n",
    "    sampled_pixels = random.choices(center_pixels_flat, k=100)\n",
    "    \n",
    "    # Count the most common color among the sampled pixels\n",
    "    color_counts = Counter(map(tuple, sampled_pixels))\n",
    "    dominant_color = color_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return dominant_color\n",
    "\n",
    "def classify_court(image_path, kmeans, cluster_centers):\n",
    "    # Extract the dominant color from the image\n",
    "    dominant_color = extract_dominant_color(image_path)\n",
    "    \n",
    "    # add feature names\n",
    "    features = [\"red\", \"green\", \"blue\"]\n",
    "    # create dataframe\n",
    "    color_df = pd.DataFrame(columns=features)\n",
    "    # add dominant color to dataframe\n",
    "    color_df = pd.concat([color_df, pd.DataFrame({\"red\": [dominant_color[0]], \"green\": [dominant_color[1]], \"blue\": [dominant_color[2]]})], ignore_index=True)\n",
    "\n",
    "    # Classify the dominant color using the KMeans model\n",
    "    label = kmeans.predict(color_df)\n",
    "    \n",
    "    # return color and label\n",
    "    return cluster_centers[label[0]], label[0]\n",
    "\n",
    "# create dataset from counter\n",
    "def create_dataset(color_counter):\n",
    "    # Create a DataFrame from the color counter\n",
    "    df = pd.DataFrame(columns=[\"red\", \"green\", \"blue\"])\n",
    "\n",
    "    # loop over counter\n",
    "    for color, count in color_counter.items():\n",
    "        # append color count times\n",
    "        for _ in range(count):\n",
    "            # add row to dataframe using concat\n",
    "            df = pd.concat([df, pd.DataFrame({\"red\": [color[0]], \"green\": [color[1]], \"blue\": [color[2]]})], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_average_color(colors):\n",
    "    # colors is an array of multiple colors arrays\n",
    "    # calculate the average color\n",
    "    avg_color = tuple(np.mean(colors, axis=0))\n",
    "    return avg_color\n",
    "\n",
    "def process_images_in_folder(folder_path):\n",
    "    color_counter = Counter()\n",
    "\n",
    "    # Collect all file paths from the folder and subfolders\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            all_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Initialize a tqdm progress bar for all files\n",
    "    with tqdm(total=len(all_files), desc=\"Processing images\") as pbar:\n",
    "        for file_path in all_files:\n",
    "            if file_path.endswith(\".jpg\"):  # Process only .jpg files\n",
    "                dominant_color = extract_dominant_color(file_path)\n",
    "                color_counter[dominant_color] += 1  # Count each dominant color\n",
    "            else:\n",
    "                print(f\"Skipping file {file_path} as it is not a .jpg file\")\n",
    "            pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    return color_counter\n",
    "\n",
    "def show_color_centers(cluster_centers):\n",
    "    # Create a plot for the color centers\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    \n",
    "    # Plot each cluster center as a color patch\n",
    "    for i, color in enumerate(cluster_centers):\n",
    "        plt.subplot(1, len(cluster_centers), i + 1)\n",
    "        plt.imshow([[color / 255]], aspect='auto')  # Divide by 255 to normalize RGB values to [0, 1]\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Cluster {i + 1}')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 8841/8841 [01:49<00:00, 80.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red</th>\n",
       "      <th>green</th>\n",
       "      <th>blue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "      <td>75</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>75</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>76</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92</td>\n",
       "      <td>78</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>78</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  red green blue\n",
       "0  92    75  120\n",
       "1  92    75  120\n",
       "2  90    76  120\n",
       "3  92    78  122\n",
       "4  92    78  122"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = process_images_in_folder(\"../00_Dataset/Court\")\n",
    "color_df = create_dataset(counts)\n",
    "color_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k= 4\n",
    "# Train a KMeans model\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(color_df)\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Show the color centers\n",
    "show_color_centers(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_court = copy.deepcopy(data)\n",
    "\n",
    "for video in data_court['subsets'][2]['videos']:\n",
    "    colors = []\n",
    "    labels = []\n",
    "    for clip in video['clips']:\n",
    "        for key,frame in clip['frames_with_objects'].items():\n",
    "            image_path = f\"../00_Dataset/Court/{video['name']}/Clip1/{key}.jpg\"\n",
    "            color, label = classify_court(image_path, kmeans, cluster_centers)\n",
    "            colors.append(color)\n",
    "            labels.append(label)\n",
    "\n",
    "    # calculate mode label\n",
    "    mode_label = max(set(labels), key=labels.count)\n",
    "    # calculate the average color\n",
    "    average_color = calculate_average_color(colors)\n",
    "\n",
    "    # update video\n",
    "    video['average_color'] = average_color\n",
    "    video['mode_label'] = int(mode_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Amateur': 247, 'Court': 1151}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove subset 1\n",
    "removed = data_court[\"subsets\"].pop(1)\n",
    "\n",
    "# loop over subset 0 and keep only every 50th frame\n",
    "for video in data_court[\"subsets\"][0][\"videos\"]:\n",
    "    for clip in video[\"clips\"]:\n",
    "        frames = list(clip[\"frames_with_objects\"].keys())\n",
    "        for frame in frames:\n",
    "            if int(frame) % 50 != 0:\n",
    "                del clip[\"frames_with_objects\"][frame]\n",
    "\n",
    "# loop over subset 1 and remove every video with a mode_label than 1\n",
    "videos_to_remove = []\n",
    "for video in data_court[\"subsets\"][1][\"videos\"]:\n",
    "    if video[\"mode_label\"] != 1:\n",
    "        videos_to_remove.append(video)\n",
    "\n",
    "for video in videos_to_remove:\n",
    "    data_court[\"subsets\"][1][\"videos\"].remove(video)\n",
    "\n",
    "# count frames for each subset\n",
    "dict={}\n",
    "for subset in data_court[\"subsets\"]:\n",
    "    dict[subset[\"name\"]] = 0\n",
    "    for video in subset[\"videos\"]:\n",
    "        for clip in video['clips']:\n",
    "            dict[subset[\"name\"]] += len(clip['frames_with_objects'].keys())\n",
    "\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: Amateur\n",
      "Split: train, total frames: 191, ratio: 0.7732793522267206\n",
      "Split: validation, total frames: 38, ratio: 0.15384615384615385\n",
      "Split: test, total frames: 18, ratio: 0.0728744939271255\n",
      "\n",
      "Subset: Court\n",
      "Split: train, total frames: 837, ratio: 0.7271937445699392\n",
      "Split: validation, total frames: 136, ratio: 0.11815812337098175\n",
      "Split: test, total frames: 178, ratio: 0.15464813205907907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign splits\n",
    "data_court = assign_splits(data_court)\n",
    "\n",
    "# count splits\n",
    "count_splits(data_court)\n",
    "\n",
    "# safe annotations\n",
    "with open('../00_Dataset/annotations_court.json', 'w') as f:\n",
    "    json.dump(data_court, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for Player Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: Amateur\n",
      "Split: train, total frames: 7889, ratio: 0.6835629494844467\n",
      "Split: validation, total frames: 1689, ratio: 0.1463478034832337\n",
      "Split: test, total frames: 1963, ratio: 0.17008924703231956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a copy of the data\n",
    "player_dataset = copy.deepcopy(data)\n",
    "\n",
    "# assign splits\n",
    "player_dataset = assign_splits(player_dataset)\n",
    "\n",
    "# remove all subsets exept first\n",
    "player_dataset[\"subsets\"].pop(1)\n",
    "player_dataset[\"subsets\"].pop(1)\n",
    "\n",
    "# count splits\n",
    "count_splits(player_dataset)\n",
    "\n",
    "# safe annotations\n",
    "with open('../00_Dataset/annotations_player.json', 'w') as f:\n",
    "    json.dump(player_dataset, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split for bounce and hit detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset: Amateur\n",
      "Split: train, total frames: 7889, ratio: 0.6835629494844467\n",
      "Split: validation, total frames: 1689, ratio: 0.1463478034832337\n",
      "Split: test, total frames: 1963, ratio: 0.17008924703231956\n",
      "\n",
      "Subset: TrackNet\n",
      "Split: train, total frames: 13947, ratio: 0.7031509957146458\n",
      "Split: validation, total frames: 2984, ratio: 0.1504411394000504\n",
      "Split: test, total frames: 2904, ratio: 0.14640786488530375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a copy of the data\n",
    "bounce_dataset = copy.deepcopy(data)\n",
    "\n",
    "# assign splits\n",
    "bounce_dataset = assign_splits(bounce_dataset)\n",
    "\n",
    "#remove third dataset by index\n",
    "bounce_dataset[\"subsets\"].pop(2)\n",
    "\n",
    "# count splits\n",
    "count_splits(bounce_dataset)\n",
    "\n",
    "# safe annotations\n",
    "with open('../00_Dataset/annotations_bounce.json', 'w') as f:\n",
    "    json.dump(bounce_dataset, f, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
