{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4273d69f",
   "metadata": {},
   "source": [
    "# Results Fully Connected\n",
    "\n",
    "In This notebook the results on the test split for the tuned Resnet50 FC model are generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dafbf58",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688dc0f-1faf-4932-a597-f23c8bd821ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from court_dataset import CourtDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002e1ac",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd91772-e64a-4bc5-9195-3e8c6fb72702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset_path, batch_size):\n",
    "    test_dataset = CourtDataset(\n",
    "        path=dataset_path,\n",
    "        split=\"test\",\n",
    "        input_height=720,\n",
    "        input_width=1280,\n",
    "        model_height=288,\n",
    "        model_width=512,\n",
    "        augment=False,\n",
    "        selected_points=None\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return test_loader, test_dataset\n",
    "\n",
    "\n",
    "def create_resnet50(num_coordinates=15):\n",
    "    # Load pretrained ResNet50\n",
    "    model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "    \n",
    "    # Modify the output layer\n",
    "    num_outputs = num_coordinates * 2  \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 256), \n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, num_outputs)           \n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def compute_mse(points, positions, img_size):\n",
    "    #loop over points\n",
    "    mse = []\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        # get the point\n",
    "        point = points[i]\n",
    "        # get the position\n",
    "        position = positions[i]\n",
    "\n",
    "        # if the point is outside the image, skip\n",
    "        if point[0] < 0 or point[0] >= img_size[0] or point[1] < 0 or point[1] >= img_size[1]:\n",
    "            continue\n",
    "\n",
    "        # if the position is -1,-1 return the maximum distance\n",
    "        if position[0] == -1 and position[1] == -1:\n",
    "            continue\n",
    "        \n",
    "        # calculate the distance\n",
    "        distance = np.linalg.norm(np.array(point) - np.array(position))\n",
    "\n",
    "        # add to mse\n",
    "        mse.append(distance**2)\n",
    "\n",
    "    # return the mean\n",
    "    return np.mean(mse)\n",
    "\n",
    "\n",
    "def compute_counts(points, positions, img_size, threshold=4):\n",
    "    # check that len is the same\n",
    "    assert len(points) == len(positions) \n",
    "    \n",
    "    #loop over points\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(len(points)):\n",
    "        # get the point\n",
    "        point = points[i]\n",
    "        # get the position\n",
    "        position = positions[i]\n",
    "        \n",
    "        # calculate the distance\n",
    "        distance = np.linalg.norm(np.array(point) - np.array(position))\n",
    "\n",
    "        # add to tp, fp, fn, tn\n",
    "        if distance <= threshold:\n",
    "            # if point is outside the frame\n",
    "            if point[0] < 0 or point[0] >= img_size[0] or point[1] < 0 or point[1] >= img_size[1]:\n",
    "                tn += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            #if point is outside the frame\n",
    "            if point[0] < 0 or point[0] >= img_size[0] or point[1] < 0 or point[1] >= img_size[1]:\n",
    "                fn += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "    # return the metrics\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "\n",
    "def calculate_metrics(tp, fp, tn, fn):\n",
    "    # Avoid division by zero for precision and recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    # F1-score calculation\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    # Accuracy calculation\n",
    "    total = tp + fp + tn + fn\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "\n",
    "def validate(model, val_loader, device, dataset, criterion=torch.nn.MSELoss(), threshold=4, img_size=(512, 288)):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0  # Initialize counts\n",
    "    losses = []\n",
    "    mse_scores = []\n",
    "    predictions = []\n",
    "\n",
    "\n",
    "    with tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "        for batch_idx, batch in enumerate(val_loader):\n",
    "            with torch.no_grad():\n",
    "                # Prepare data\n",
    "                inputs = batch[0].float().to(device)  # Input images\n",
    "                keypoints_gt = batch[1].float().to(device)  # Ground truth keypoints\n",
    "                idx = batch[2].cpu().numpy()  # Sample indices\n",
    "\n",
    "                outputs = model(inputs)  # Predicted keypoints\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, keypoints_gt.view(outputs.size()))  # Match shapes for regression\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                # First loop: Iterate over each item in the batch\n",
    "                for i in range(outputs.shape[0]):  # Loop over batch samples\n",
    "                    # get predictions\n",
    "                    positions = outputs[i].view(-1, 2).cpu().numpy()\n",
    "                    # move to cpu\n",
    "                    keypoints = keypoints_gt[i].cpu().numpy()\n",
    "\n",
    "                    # get entry from dataset\n",
    "                    entry = dataset.data.iloc[idx[i]].to_dict()\n",
    "                    \n",
    "                    #Compute MSE for the batch\n",
    "                    mse = compute_mse(keypoints, positions, img_size)\n",
    "                    mse_scores.append(mse)\n",
    "\n",
    "                    predictions.append({\n",
    "                        \"idx\": idx[i],\n",
    "                        \"points_transformed\": keypoints,\n",
    "                        \"positions\": positions,\n",
    "                        \"mse\":mse,\n",
    "                        **entry})\n",
    "\n",
    "                    # Compute TP, FP, TN, FN for the batch\n",
    "                    item_tp, item_fp, item_tn, item_fn = compute_counts(keypoints, positions, img_size, threshold)\n",
    "                    tp += item_tp\n",
    "                    fp += item_fp\n",
    "                    tn += item_tn\n",
    "                    fn += item_fn\n",
    "\n",
    "                # Update the tqdm bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': round(np.mean(losses), 6)\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    mean_loss = np.mean(losses)\n",
    "    mean_mse = np.mean(mse_scores)\n",
    "    metrics = calculate_metrics(tp, fp, tn, fn)\n",
    "\n",
    "    #convert predictions to dataframe\n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "    return mean_loss, mean_mse, metrics, predictions_df\n",
    "\n",
    "\n",
    "def load_model(best_model_path):\n",
    "    selected_gpus = [0, 1]\n",
    "    device = torch.device(f'cuda:{selected_gpus[0]}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Recreate the model architecture\n",
    "    model = create_resnet50(15)\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))  # Load weights\n",
    "    model = model.to(device)  # Move the model to the specified device\n",
    "\n",
    "    print(f\"Model loaded from: {best_model_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabde82",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47d39c-7938-46c2-b036-31fbb8c3405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_299077/3575687890.py:191: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=device))  # Load weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: ./exps_hyperparameter/serene-sweep-10_resnet50_adam_l1/model_best.pth\n",
      "Samples: 199\n"
     ]
    }
   ],
   "source": [
    "best_model_path = \"./exps_hyperparameter/serene-sweep-10_resnet50_adam_l1/model_best.pth\"\n",
    "\n",
    "model = load_model(best_model_path)\n",
    "\n",
    "# create dataloader\n",
    "dataset_path = \"../../00_Dataset\"\n",
    "batch_size = 32\n",
    "val_loader, val_dataset = create_dataloader(dataset_path, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d115a871-8079-49cc-87a0-f3971af7f8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 7/7 [00:01<00:00,  4.98batch/s, loss=146]\n"
     ]
    }
   ],
   "source": [
    "selected_gpus = [0,1]\n",
    "device = torch.device(f'cuda:{selected_gpus[0]}')\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# validate the model\n",
    "mean_loss, mean_mse, metrics, predictions_df = validate(model, val_loader, device, val_dataset, criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993e129b-1848-4bb0-a58f-8b71153a64c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145.63202503749304\n",
      "221.2128\n",
      "{'accuracy': 0.2576214405360134, 'precision': 0.2606924643584521, 'recall': 0.9528535980148883, 'f1': 0.40938166311300633}\n"
     ]
    }
   ],
   "source": [
    "print(mean_loss)\n",
    "print(mean_mse)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03f70c8-5330-4e4c-a3eb-4b44bddc8416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>points_transformed</th>\n",
       "      <th>positions</th>\n",
       "      <th>mse</th>\n",
       "      <th>subset</th>\n",
       "      <th>video</th>\n",
       "      <th>clip</th>\n",
       "      <th>frame</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[179.2, 179.2], [197.6, 178.40001], [300.0, 1...</td>\n",
       "      <td>[[152.11049, 174.79532], [175.00192, 174.58383...</td>\n",
       "      <td>1167.539917</td>\n",
       "      <td>New</td>\n",
       "      <td>Video_1</td>\n",
       "      <td>clip_6</td>\n",
       "      <td>250</td>\n",
       "      <td>{'top_left_corner': [448, 448], 'top_left_sing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[179.2, 179.2], [197.6, 178.40001], [300.0, 1...</td>\n",
       "      <td>[[153.07959, 175.55824], [175.8067, 175.34052]...</td>\n",
       "      <td>1168.081787</td>\n",
       "      <td>New</td>\n",
       "      <td>Video_1</td>\n",
       "      <td>clip_6</td>\n",
       "      <td>300</td>\n",
       "      <td>{'top_left_corner': [448, 448], 'top_left_sing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[179.2, 179.2], [197.6, 178.40001], [300.0, 1...</td>\n",
       "      <td>[[152.45457, 175.63797], [175.21523, 175.41908...</td>\n",
       "      <td>1167.584839</td>\n",
       "      <td>New</td>\n",
       "      <td>Video_1</td>\n",
       "      <td>clip_6</td>\n",
       "      <td>350</td>\n",
       "      <td>{'top_left_corner': [448, 448], 'top_left_sing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[197.6, 81.6], [208.8, 81.6], [273.6, 80.4], ...</td>\n",
       "      <td>[[203.7736, 83.53658], [218.14622, 83.23999], ...</td>\n",
       "      <td>1125.257324</td>\n",
       "      <td>New</td>\n",
       "      <td>Video_2</td>\n",
       "      <td>clip_7</td>\n",
       "      <td>300</td>\n",
       "      <td>{'top_left_corner': [494, 204], 'top_left_sing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[197.6, 81.6], [208.8, 81.6], [273.6, 80.4], ...</td>\n",
       "      <td>[[203.29466, 83.29217], [217.722, 82.995384], ...</td>\n",
       "      <td>1129.026611</td>\n",
       "      <td>New</td>\n",
       "      <td>Video_2</td>\n",
       "      <td>clip_7</td>\n",
       "      <td>350</td>\n",
       "      <td>{'top_left_corner': [494, 204], 'top_left_sing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                 points_transformed  \\\n",
       "0    0  [[179.2, 179.2], [197.6, 178.40001], [300.0, 1...   \n",
       "1    1  [[179.2, 179.2], [197.6, 178.40001], [300.0, 1...   \n",
       "2    2  [[179.2, 179.2], [197.6, 178.40001], [300.0, 1...   \n",
       "3    3  [[197.6, 81.6], [208.8, 81.6], [273.6, 80.4], ...   \n",
       "4    4  [[197.6, 81.6], [208.8, 81.6], [273.6, 80.4], ...   \n",
       "\n",
       "                                           positions          mse subset  \\\n",
       "0  [[152.11049, 174.79532], [175.00192, 174.58383...  1167.539917    New   \n",
       "1  [[153.07959, 175.55824], [175.8067, 175.34052]...  1168.081787    New   \n",
       "2  [[152.45457, 175.63797], [175.21523, 175.41908...  1167.584839    New   \n",
       "3  [[203.7736, 83.53658], [218.14622, 83.23999], ...  1125.257324    New   \n",
       "4  [[203.29466, 83.29217], [217.722, 82.995384], ...  1129.026611    New   \n",
       "\n",
       "     video    clip frame                                             points  \n",
       "0  Video_1  clip_6   250  {'top_left_corner': [448, 448], 'top_left_sing...  \n",
       "1  Video_1  clip_6   300  {'top_left_corner': [448, 448], 'top_left_sing...  \n",
       "2  Video_1  clip_6   350  {'top_left_corner': [448, 448], 'top_left_sing...  \n",
       "3  Video_2  clip_7   300  {'top_left_corner': [494, 204], 'top_left_sing...  \n",
       "4  Video_2  clip_7   350  {'top_left_corner': [494, 204], 'top_left_sing...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878796a9-2e1d-44b5-836d-5711d03491e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(\"results/FC_test_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
